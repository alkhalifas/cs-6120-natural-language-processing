{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75319e16",
   "metadata": {
    "id": "75319e16"
   },
   "source": [
    "# What is text preprocessing in NLP and why it is important?\n",
    "\n",
    "<b>Text preprocessing is a first step in any text based classification. In large corpus, text comes from various resources which contain noise along with valueable information. If we use unprocessed data directly with the models, it model will peform badly and give unpredictable result.<b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9758275d",
   "metadata": {
    "id": "9758275d"
   },
   "source": [
    "### Steps to install NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740efc78",
   "metadata": {
    "id": "740efc78"
   },
   "source": [
    "**Mac/Unix**\n",
    "\n",
    "From the terminal:\n",
    "1. Install NLTK: run `pip install -U nltk`\n",
    "2. Test installation: run `python` then type `import nltk`\n",
    "\n",
    "**Windows**\n",
    "\n",
    "1. Install NLTK: [http://pypi.python.org/pypi/nltk](http://pypi.python.org/pypi/nltk)\n",
    "2. Test installation: `Start>Python35`, then type `import nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6d43e",
   "metadata": {
    "id": "5da6d43e"
   },
   "source": [
    "### Download NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34204879",
   "metadata": {
    "id": "34204879"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f399370",
   "metadata": {
    "id": "6f399370"
   },
   "source": [
    "### 1) Convert all words to lower case\n",
    "<b>Lower and upper case for same words is treated as different by the models, so we need to convert all words to lower case or uppper case<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204caf78",
   "metadata": {
    "id": "204caf78"
   },
   "outputs": [],
   "source": [
    "def toLowerCase(text):\n",
    "    return text.lower() #changes all upper case alphabet to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ed92b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11ed92b5",
    "outputId": "f24e5678-c29f-4eb1-9611-1c74a0028eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - Did you catch the bus ? Are you frying an egg ?\n",
      "After  - did you catch the bus ? are you frying an egg ?\n"
     ]
    }
   ],
   "source": [
    "text = 'Did you catch the bus ? Are you frying an egg ?'\n",
    "print(f'Before - {text}')\n",
    "print(f'After  - {toLowerCase(text)}')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c47efbd",
   "metadata": {
    "id": "9c47efbd"
   },
   "source": [
    "### 2) Removal of URLs\n",
    "<b> Regular expression or RegEx in Python is denoted as RE (REs, regexes or regex pattern) are imported through re module. The functions in this module let you check if a particular string matches a given regular expression.<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07bd2c23",
   "metadata": {
    "id": "07bd2c23"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def removeURLs(text):\n",
    "    \n",
    "    text = re.sub(r\"http\\S+\", \"\", text) # replaces URLs starting with http \n",
    "    text = re.sub(r\"www.\\S+\", \"\", text) # replaces URLs starting with wwe\n",
    "    text = re.sub(r\"\\S+.com$\", \"\", text) # replaces URLs ending with .com\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94bafa03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94bafa03",
    "outputId": "d56dfbb7-5c8d-462a-ace6-cd400ab8559f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - https://thistutorialisawesome.com is best site, but www.paperlinks.tech is better, but I beg you might also get google.com\n",
      "After  -  is best site, but  is better, but I beg you might also get \n"
     ]
    }
   ],
   "source": [
    "text = 'https://thistutorialisawesome.com is best site, but www.paperlinks.tech is better, but I beg you might also get google.com'\n",
    "print(f'Before - {text}')\n",
    "print(f'After  - {removeURLs(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7413df",
   "metadata": {
    "id": "ba7413df"
   },
   "source": [
    "### 3) Removal of punctuations\n",
    "<b> Punctuation don't add any value to overall text and can be removed without impacting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f503f9db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "f503f9db",
    "outputId": "f5bda600-e88b-4a69-fbb4-209f2435497a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation # checking punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e06106",
   "metadata": {
    "id": "09e06106"
   },
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f271ccd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f271ccd",
    "outputId": "c6a98797-ff48-4766-ce3c-02a146f5919f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - Is that seriously how you spell my name? I am not joking.\n",
      "After  - Is that seriously how you spell my name I am not joking\n"
     ]
    }
   ],
   "source": [
    "text = 'Is that seriously how you spell my name? I am not joking.'\n",
    "print(f'Before - {text}')\n",
    "print(f'After  - {removePunctuation(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201ecaa",
   "metadata": {
    "id": "8201ecaa"
   },
   "source": [
    "### 4) Removal of stopwords\n",
    "<b>In sentiment analysis, stopwords like pronouns can overweight actual words expressing emotions, which leads to poor for perfomance of model. So stopwords should be removed as a part of text preprocessing<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50f35e6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50f35e6f",
    "outputId": "9aa84fd6-0679-4d7c-cad4-00e441a477cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'herself', 'been', 'with', 'here', 'very', 'doesn', 'won']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Saleh\n",
      "[nltk_data]     Alkhalifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "print(stopword[0:500:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd2caa28",
   "metadata": {
    "id": "fd2caa28"
   },
   "outputs": [],
   "source": [
    "def removeStopwords(text):\n",
    "    return \" \".join([word for word in re.split('\\W+', text)\n",
    "        if word not in stopword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d5bb0cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d5bb0cf",
    "outputId": "97f46513-a5f2-4663-c47f-36189228f572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - is that seriously how you spell my name? i am not joking.\n",
      "After  - seriously spell name joking \n"
     ]
    }
   ],
   "source": [
    "text = 'Is that seriously how you spell my name? I am not joking.'\n",
    "print(f'Before - {text.lower()}')\n",
    "print(f'After  - {removeStopwords(text.lower())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0dc67",
   "metadata": {
    "id": "bcf0dc67"
   },
   "source": [
    "### 5) Tokenization - It is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n",
    "\n",
    "<b>NLTK contains a module called tokenize() which further classifies into two sub-categories: \n",
    "Word tokenize: We use the word_tokenize() method to split a sentence into tokens or words.\n",
    "Sentence tokenize: We use the sent_tokenize() method to split a document or paragraph into sentences.<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e741b26d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e741b26d",
    "outputId": "280e2c30-cae3-434b-ea89-e9e310429bc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Saleh\n",
      "[nltk_data]     Alkhalifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def sentenceTokenize(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def wordTokenize(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1978a79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1978a79",
    "outputId": "02a9dc31-11f7-4637-ee64-24436fc3ccf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - Its a wonderful day in Boston. I really like NLP. I will be visting some nearby park today.\n",
      "After  - ['its a wonderful day in boston.', 'i really like nlp.', 'i will be visting some nearby park today.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Its a wonderful day in Boston. I really like NLP. I will be visting some nearby park today.'\n",
    "print(f'Before - {text}')\n",
    "print(f'After  - {sentenceTokenize(text.lower())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c386674",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c386674",
    "outputId": "0c9854f7-9095-445d-f455-c2a90cadca9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - Its a wonderful day in Boston. I really NLP. I will be visting some nearby park today.\n",
      "After  - ['its', 'a', 'wonderful', 'day', 'in', 'boston', '.', 'i', 'really', 'nlp', '.', 'i', 'will', 'be', 'visting', 'some', 'nearby', 'park', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Its a wonderful day in Boston. I really NLP. I will be visting some nearby park today.'\n",
    "print(f'Before - {text}')\n",
    "print(f'After  - {wordTokenize(text.lower())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fcfc8",
   "metadata": {
    "id": "5f8fcfc8"
   },
   "source": [
    "### 6) Stemming \n",
    "<b>Stemming is the process of reducing words to their stem/roots. It help in removing redundant words. For eg. root of connect, connected, connecting, connection is same.\n",
    "<b><b>We will be using NLTK to perform this task<b>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9332bf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9332bf0",
    "outputId": "a77d81e6-9284-4260-ffec-0c0cb523a435"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Saleh\n",
      "[nltk_data]     Alkhalifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04ef395",
   "metadata": {
    "id": "c04ef395"
   },
   "outputs": [],
   "source": [
    "def performStemming(text):\n",
    "     return\" \".join([ps.stem(word) for word in re.split('\\W+', text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6d56e67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6d56e67",
    "outputId": "4590c10c-8252-47d8-940a-30d00a6ca196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - It am troubled by riding bicyle daily. I have no troubles taking a ride in lyft to college.\n",
      "After  - it am troubl by ride bicyl daili i have no troubl take a ride in lyft to colleg \n"
     ]
    }
   ],
   "source": [
    "text = 'It am troubled by riding bicyle daily. I have no troubles taking a ride in lyft to college.'\n",
    "print(f'Before - {text}')\n",
    "print(f'After  - {performStemming(text.lower())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y2lBiEki3_GP",
   "metadata": {
    "id": "y2lBiEki3_GP"
   },
   "source": [
    "### 7) Lemmatization\n",
    "<b>It is similar to stemming except the lemmatized word belongs to the language. It also allows us to specify verb or noun to be used as parameter.<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77e3dba6",
   "metadata": {
    "id": "77e3dba6"
   },
   "outputs": [],
   "source": [
    "def performLemmatization(text):\n",
    "     return\" \".join([wn.lemmatize(word,'v') for word in re.split('\\W+', text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e06fbcc7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e06fbcc7",
    "outputId": "20f87b00-118d-4209-e4fa-ef4e64c9ffe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - It am troubled by riding bicyle daily. I have no troubles taking a ride in lyft to college.\n",
      "After  - it be trouble by rid bicyle daily i have no trouble take a ride in lyft to college \n"
     ]
    }
   ],
   "source": [
    "text = 'It am troubled by riding bicyle daily. I have no troubles taking a ride in lyft to college.'\n",
    "print(f'Before - {text}')\n",
    "print(f'After  - {performLemmatization(text.lower())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e866b-7b56-4f4c-b803-8709b6c8f7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Preprocessing using NLTK library.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
